# CDN BILLING & USAGE MONITORING SYSTEM - IMPLEMENTATION PLAN

**Project**: Bitmovin CDN Challenge - Live Coding Interview
**Complexity Level**: Level 4 (Advanced - Multi-subsystem, Distributed Systems)
**Estimated Time**: 4 hours
**Status**: PLANNING COMPLETE â†’ Ready for IMPLEMENTATION

---

## ğŸ“‹ TABLE OF CONTENTS

1. [Requirements Analysis](#requirements-analysis)
2. [Architecture Overview](#architecture-overview)
3. [Implementation Phases](#implementation-phases)
4. [High Availability Strategy](#high-availability-strategy)
5. [Challenges & Mitigations](#challenges--mitigations)
6. [Testing Strategy](#testing-strategy)
7. [Observability Plan](#observability-plan)
8. [Implementation Checklist](#implementation-checklist)

---

## ğŸ¯ REQUIREMENTS ANALYSIS

### Functional Requirements

#### 1. Billing Integration
- Send CDN usage events to billing system **every 24 hours**
- Use accurate AWS Cost Explorer data (24h delayed)
- Event format:
  ```json
  {
    "type": "EVENT_CDN_USAGE",
    "payload": {
      "id": "<identifier>",
      "customerId": "...",
      "startPeriod": "<start timestamp>",
      "endPeriod": "<end timestamp>",
      "trafficUsageGb": "<traffic within period in GB>"
    }
  }
  ```
- For challenge: `System.out.println()` to console
- In production: Would publish to message broker (Kafka)

#### 2. Customer REST API
- Endpoint: `GET /api/usage`
- Header: `X-Customer-Id` (guaranteed by upstream microservice)
- Query params: `from` (optional), `to` (optional)
- Response: Usage data with **24-hour resolution** (one data point per day)
- Data delay: **Maximum 15 minutes**
- Use CloudWatch for recent data (< 24h ago)
- Use Cost Explorer for historical data (> 24h ago)

#### 3. Usage Capping (Auto-Disable CDN)
- **Limit 1**: 100 GB within 15 minutes â†’ Disable CDN
- **Limit 2**: 500 GB within 3 hours â†’ Disable CDN
- Data source: AWS CloudWatch (3-hour window, eventually consistent)
- Actions when exceeded:
  1. Set distribution status to DISABLED
  2. Record disable timestamp and reason
  3. Publish event to message broker (mock with `System.out.println()`)

#### 4. Data Retrieval Strategy
| Data Source | Delay | Accuracy | Use Case |
|-------------|-------|----------|----------|
| **Cost Explorer** | 24 hours | Exact (byte-level) | Billing, Historical API |
| **CloudWatch** | 3 hours | Approximate | Capping, Recent API |

### Non-Functional Requirements

#### 1. High Availability (HA)
- **Critical**: Multiple instances running concurrently
- **Challenge**: Prevent duplicate billing events
- **Challenge**: Prevent duplicate capping actions
- **Solution**: Distributed locking (ShedLock) + idempotency

#### 2. Observability
- **Logging**: Structured logs for debugging and auditing
- **Metrics**: Prometheus-compatible metrics via Micrometer
- **Health**: Actuator health checks for AWS data sources
- **Monitoring**: Track billing events, capping events, API usage

#### 3. Resilience
- Handle AWS client failures gracefully
- Retry mechanisms for transient failures
- Fallback strategies when data unavailable

---

## ğŸ—ï¸ ARCHITECTURE OVERVIEW

### System Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                CDN CHALLENGE APPLICATION (Spring Boot)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   REST API Layer     â”‚          â”‚   Scheduled Tasks     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ UsageControllerâ”‚  â”‚          â”‚  â”‚  BillingJob    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚  â”‚  (24h cadence) â”‚  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”‚ ExceptionHandlerâ”‚ â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚  â”‚  CappingJob    â”‚  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚  (1min cadence)â”‚  â”‚    â”‚
â”‚           â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚           â”‚                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚           â”‚                         â”‚  â”‚ CollectionJob  â”‚  â”‚    â”‚
â”‚           â”‚                         â”‚  â”‚  (5min cadence)â”‚  â”‚    â”‚
â”‚           â”‚                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚           â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                  â”‚                   â”‚
â”‚           â–¼                                  â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         UsageOrchestrationService                       â”‚    â”‚
â”‚  â”‚  - Coordinates data retrieval                           â”‚    â”‚
â”‚  â”‚  - Selects appropriate data source                      â”‚    â”‚
â”‚  â”‚  - Aggregates usage across time periods                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                  â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ CloudWatchService   â”‚          â”‚ CostExplorerService  â”‚    â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚ â”‚3h window data   â”‚ â”‚          â”‚ â”‚24h delayed data â”‚  â”‚    â”‚
â”‚  â”‚ â”‚Eventually cons. â”‚ â”‚          â”‚ â”‚Byte-accurate    â”‚  â”‚    â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚        â–¼            â”‚          â”‚         â–¼            â”‚    â”‚
â”‚  â”‚ CloudWatchClient    â”‚          â”‚ CostExplorerClient   â”‚    â”‚
â”‚  â”‚ (Mock provided)     â”‚          â”‚ (Mock provided)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                  â”‚                   â”‚
â”‚           â–¼                                  â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           Data Aggregation & Storage Layer             â”‚    â”‚
â”‚  â”‚                                                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚   Customer   â”‚  â”‚ Distribution  â”‚  â”‚UsageSnapshotâ”‚ â”‚    â”‚
â”‚  â”‚  â”‚              â”‚  â”‚               â”‚  â”‚             â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ customer_id  â”‚â—„â”€â”¤ distribution  â”‚â—„â”€â”¤ time-series â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ name         â”‚  â”‚ customer_id   â”‚  â”‚ data        â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ created_at   â”‚  â”‚ bucket_id     â”‚  â”‚ 5min grain  â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ status        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚                    â”‚ disabled_at   â”‚                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚BillingEvent  â”‚                      â”‚  ShedLock   â”‚ â”‚    â”‚
â”‚  â”‚  â”‚              â”‚                      â”‚             â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ Idempotency  â”‚                      â”‚ HA locking  â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ tracking     â”‚                      â”‚ mechanism   â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                     â”‚
â”‚           â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Integration Layer                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ BillingEvent      â”‚  â”‚ CdnDisable             â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Publisher         â”‚  â”‚ Service                â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                   â”‚  â”‚                        â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ System.out.print  â”‚  â”‚ System.out.print       â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚            Observability Layer                          â”‚    â”‚
â”‚  â”‚  - Structured Logging (SLF4J)                          â”‚    â”‚
â”‚  â”‚  - Metrics (Micrometer)                                â”‚    â”‚
â”‚  â”‚  - Health Checks (Actuator)                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

- **Framework**: Spring Boot 3.0.0
- **Java Version**: 17
- **Database**: H2 (in-memory for challenge, PostgreSQL for production)
- **Scheduling**: Spring `@Scheduled` + ShedLock
- **Observability**: Spring Actuator + Micrometer
- **AWS SDK**: CloudWatch 2.18.17, Cost Explorer 2.17.121
- **Testing**: JUnit 5, Mockito, Spring Boot Test

---

## ğŸ“ IMPLEMENTATION PHASES

### Phase 1: Domain Model & Database Schema (30 min)

#### Entities to Create

**1.1 Customer Entity**
```java
@Entity
@Table(name = "customer")
public class Customer {
    @Id
    @Column(name = "customer_id")
    private String customerId;
    
    private String name;
    
    @Column(name = "created_at")
    private Instant createdAt;
    
    @OneToMany(mappedBy = "customer")
    private List<Distribution> distributions;
}
```

**1.2 Distribution Entity**
```java
@Entity
@Table(name = "distribution")
public class Distribution {
    @Id
    @Column(name = "distribution_id")
    private String distributionId;
    
    @ManyToOne
    @JoinColumn(name = "customer_id")
    private Customer customer;
    
    @Column(name = "bucket_id")
    private String bucketId;
    
    @Enumerated(EnumType.STRING)
    private DistributionStatus status; // ACTIVE, DISABLED
    
    @Column(name = "disabled_at")
    private Instant disabledAt;
    
    @Column(name = "disable_reason")
    private String disableReason;
    
    @Version
    private Long version; // Optimistic locking for HA
}
```

**1.3 UsageSnapshot Entity (Time-Series)**
```java
@Entity
@Table(name = "usage_snapshot", indexes = {
    @Index(name = "idx_customer_time", columnList = "customer_id,snapshot_time"),
    @Index(name = "idx_dist_time_source", columnList = "distribution_id,snapshot_time,source")
})
public class UsageSnapshot {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(name = "distribution_id")
    private String distributionId;
    
    @Column(name = "customer_id")
    private String customerId;
    
    @Column(name = "snapshot_time")
    private Instant snapshotTime;
    
    @Column(name = "data_transfer_gb")
    private Double dataTransferGb;
    
    @Enumerated(EnumType.STRING)
    private UsageDataSource source; // CLOUDWATCH, COST_EXPLORER
    
    @Column(name = "period_start")
    private Instant periodStart;
    
    @Column(name = "period_end")
    private Instant periodEnd;
}
```

**1.4 BillingEvent Entity (Idempotency Tracking)**
```java
@Entity
@Table(name = "billing_event", uniqueConstraints = {
    @UniqueConstraint(name = "uk_billing", 
                     columnNames = {"customer_id", "period_start", "period_end"})
})
public class BillingEvent {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(name = "customer_id")
    private String customerId;
    
    @Column(name = "period_start")
    private Instant periodStart;
    
    @Column(name = "period_end")
    private Instant periodEnd;
    
    @Column(name = "traffic_usage_gb")
    private Double trafficUsageGb;
    
    @Column(name = "sent_at")
    private Instant sentAt;
    
    @Enumerated(EnumType.STRING)
    private BillingEventStatus status; // PENDING, SENT, FAILED
}
```

#### Repositories

**1.5 Create Spring Data JPA Repositories**
```java
public interface CustomerRepository extends JpaRepository<Customer, String> {
}

public interface DistributionRepository extends JpaRepository<Distribution, String> {
    List<Distribution> findByStatus(DistributionStatus status);
    List<Distribution> findByCustomer_CustomerId(String customerId);
}

public interface UsageSnapshotRepository extends JpaRepository<UsageSnapshot, Long> {
    @Query("SELECT u FROM UsageSnapshot u WHERE u.customerId = :customerId " +
           "AND u.snapshotTime >= :from AND u.snapshotTime <= :to " +
           "ORDER BY u.snapshotTime")
    List<UsageSnapshot> findByCustomerAndTimeRange(
        @Param("customerId") String customerId,
        @Param("from") Instant from,
        @Param("to") Instant to
    );
    
    @Query("SELECT SUM(u.dataTransferGb) FROM UsageSnapshot u " +
           "WHERE u.distributionId = :distributionId " +
           "AND u.snapshotTime >= :from AND u.snapshotTime <= :to " +
           "AND u.source = :source")
    Double sumUsageByDistributionAndTimeRange(
        @Param("distributionId") String distributionId,
        @Param("from") Instant from,
        @Param("to") Instant to,
        @Param("source") UsageDataSource source
    );
}

public interface BillingEventRepository extends JpaRepository<BillingEvent, Long> {
    Optional<BillingEvent> findByCustomerIdAndPeriodStartAndPeriodEnd(
        String customerId, Instant periodStart, Instant periodEnd
    );
    
    @Query("SELECT MAX(b.periodEnd) FROM BillingEvent b " +
           "WHERE b.customerId = :customerId AND b.status = 'SENT'")
    Optional<Instant> findLastBillingPeriodEnd(@Param("customerId") String customerId);
}
```

#### Database Schema

**1.6 schema.sql**
```sql
-- Customer entity removed - using customer_id from header
-- CREATE TABLE customer (
    customer_id VARCHAR(255) PRIMARY KEY,
    name VARCHAR(255),
    created_at TIMESTAMP
);

CREATE TABLE distribution (
    distribution_id VARCHAR(255) PRIMARY KEY,
    customer_id VARCHAR(255) NOT NULL,
    bucket_id VARCHAR(255),
    status VARCHAR(50) NOT NULL,
    disabled_at TIMESTAMP,
    disable_reason VARCHAR(500),
    version BIGINT NOT NULL DEFAULT 0,
    FOREIGN KEY (customer_id) REFERENCES customer(customer_id)
);

CREATE TABLE usage_snapshot (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    distribution_id VARCHAR(255) NOT NULL,
    customer_id VARCHAR(255) NOT NULL,
    snapshot_time TIMESTAMP NOT NULL,
    data_transfer_gb DOUBLE NOT NULL,
    source VARCHAR(50) NOT NULL,
    period_start TIMESTAMP NOT NULL,
    period_end TIMESTAMP NOT NULL,
    FOREIGN KEY (distribution_id) REFERENCES distribution(distribution_id)
);

CREATE INDEX idx_customer_time ON usage_snapshot(customer_id, snapshot_time);
CREATE INDEX idx_dist_time_source ON usage_snapshot(distribution_id, snapshot_time, source);

CREATE TABLE billing_event (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    customer_id VARCHAR(255) NOT NULL,
    period_start TIMESTAMP NOT NULL,
    period_end TIMESTAMP NOT NULL,
    traffic_usage_gb DOUBLE NOT NULL,
    sent_at TIMESTAMP,
    status VARCHAR(50) NOT NULL,
    CONSTRAINT uk_billing UNIQUE (customer_id, period_start, period_end)
);

-- ShedLock table for distributed locking
CREATE TABLE shedlock (
    name VARCHAR(64) PRIMARY KEY,
    lock_until TIMESTAMP NOT NULL,
    locked_at TIMESTAMP NOT NULL,
    locked_by VARCHAR(255) NOT NULL
);
```

**1.7 data.sql (Seed Data)**
```sql
-- Seed customers
INSERT INTO customer (customer_id, name, created_at) VALUES
('customer#1', 'Acme Corporation', CURRENT_TIMESTAMP),
('customer#2', 'TechStart Inc', CURRENT_TIMESTAMP),
('customer#3', 'MediaStream Ltd', CURRENT_TIMESTAMP);

-- Seed distributions (map to mock data in CloudWatchClient/CostExplorerClient)
INSERT INTO distribution (distribution_id, customer_id, bucket_id, status, version) VALUES
('dist-uuid-1', 'customer#1', 'bucket#1', 'ACTIVE', 0),
('dist-uuid-2', 'customer#2', 'bucket#2', 'ACTIVE', 0),
('dist-uuid-3', 'customer#1', 'bucket#3', 'ACTIVE', 0);
```

**Deliverables:**
- [ ] 4 JPA entities with proper relationships
- [ ] 4 Spring Data repositories with custom queries
- [ ] schema.sql with indexes and constraints
- [ ] data.sql with seed data
- [ ] Enums: DistributionStatus, UsageDataSource, BillingEventStatus

---

### Phase 2: AWS Data Retrieval Services (45 min)

#### 2.1 CloudWatchService

**Purpose**: Retrieve usage data from CloudWatch (3-hour window, eventually consistent)

```java
@Service
@Slf4j
public class CloudWatchService {
    
    private final CloudWatchClient cloudWatchClient;
    private final UsageSnapshotRepository usageSnapshotRepository;
    private final DistributionRepository distributionRepository;
    
    /**
     * Retrieve recent usage from CloudWatch and store as snapshots
     * @param from Start time (typically NOW - 3 hours)
     * @param to End time (typically NOW)
     * @return List of created usage snapshots
     */
    public List<UsageSnapshot> retrieveAndStoreRecentUsage(Instant from, Instant to) {
        log.info("Retrieving CloudWatch metrics from {} to {}", from, to);
        
        GetMetricDataResponse response = cloudWatchClient.getMetrics(from, to);
        List<UsageSnapshot> snapshots = new ArrayList<>();
        
        for (MetricDataResult result : response.metricDataResults()) {
            String distributionId = extractDistributionId(result.label());
            Distribution distribution = distributionRepository.findById(distributionId)
                .orElse(null);
            
            if (distribution == null) {
                log.warn("Distribution not found: {}", distributionId);
                continue;
            }
            
            // CloudWatch returns values in bytes, convert to GB
            Double usageGb = result.values().stream()
                .mapToDouble(Double::doubleValue)
                .sum() / (1024.0 * 1024.0 * 1024.0);
            
            UsageSnapshot snapshot = new UsageSnapshot();
            snapshot.setDistributionId(distributionId);
            snapshot.setCustomerId(distribution.getCustomer().getCustomerId());
            snapshot.setSnapshotTime(Instant.now());
            snapshot.setDataTransferGb(usageGb);
            snapshot.setSource(UsageDataSource.CLOUDWATCH);
            snapshot.setPeriodStart(from);
            snapshot.setPeriodEnd(to);
            
            snapshots.add(usageSnapshotRepository.save(snapshot));
        }
        
        log.info("Stored {} CloudWatch snapshots", snapshots.size());
        return snapshots;
    }
    
    /**
     * Calculate usage for a distribution within a sliding window
     * Used for usage capping checks (15min, 3hr windows)
     */
    public Double getUsageForWindow(String distributionId, Duration window) {
        Instant to = Instant.now();
        Instant from = to.minus(window);
        
        Double usage = usageSnapshotRepository.sumUsageByDistributionAndTimeRange(
            distributionId, from, to, UsageDataSource.CLOUDWATCH
        );
        
        return usage != null ? usage : 0.0;
    }
    
    /**
     * Check if distribution usage exceeds threshold within window
     */
    public boolean exceedsThreshold(String distributionId, Double thresholdGb, Duration window) {
        Double usage = getUsageForWindow(distributionId, window);
        boolean exceeds = usage >= thresholdGb;
        
        if (exceeds) {
            log.warn("Distribution {} exceeded threshold: {} GB (limit: {} GB) in {}",
                distributionId, usage, thresholdGb, window);
        }
        
        return exceeds;
    }
    
    private String extractDistributionId(String label) {
        // Label format: "Distribution dist-uuid-1"
        return label.substring(label.lastIndexOf(" ") + 1);
    }
}
```

#### 2.2 CostExplorerService

**Purpose**: Retrieve accurate billing data from Cost Explorer (24h delayed)

```java
@Service
@Slf4j
public class CostExplorerService {
    
    private final CostExplorerClient costExplorerClient;
    private final UsageSnapshotRepository usageSnapshotRepository;
    private final DistributionRepository distributionRepository;
    
    /**
     * Retrieve billing-accurate usage from Cost Explorer
     * Note: Data is delayed by ~24 hours
     */
    public List<UsageSnapshot> retrieveAndStoreBillingUsage(Instant from, Instant to) {
        log.info("Retrieving Cost Explorer data from {} to {}", from, to);
        
        List<UsageSnapshot> snapshots = new ArrayList<>();
        List<Distribution> distributions = distributionRepository.findAll();
        
        for (Distribution distribution : distributions) {
            GetCostAndUsageResponse response = costExplorerClient.retrieveUsage(
                from, to, distribution.getDistributionId()
            );
            
            Double dataTransferBytes = extractDataTransferBytes(response);
            Double usageGb = dataTransferBytes / (1024.0 * 1024.0 * 1024.0);
            
            UsageSnapshot snapshot = new UsageSnapshot();
            snapshot.setDistributionId(distribution.getDistributionId());
            snapshot.setCustomerId(distribution.getCustomer().getCustomerId());
            snapshot.setSnapshotTime(Instant.now());
            snapshot.setDataTransferGb(usageGb);
            snapshot.setSource(UsageDataSource.COST_EXPLORER);
            snapshot.setPeriodStart(from);
            snapshot.setPeriodEnd(to);
            
            snapshots.add(usageSnapshotRepository.save(snapshot));
        }
        
        log.info("Stored {} Cost Explorer snapshots", snapshots.size());
        return snapshots;
    }
    
    /**
     * Get aggregated usage per customer for billing
     */
    public Map<String, Double> getCustomerUsageForBilling(Instant from, Instant to) {
        Map<String, Double> customerUsage = new HashMap<>();
        
        List<Customer> customers = customerRepository.findAll();
        
        for (Customer customer : customers) {
            Double totalUsage = usageSnapshotRepository
                .sumUsageByCustomerAndTimeRange(
                    customer.getCustomerId(), from, to, UsageDataSource.COST_EXPLORER
                );
            
            customerUsage.put(customer.getCustomerId(), totalUsage != null ? totalUsage : 0.0);
        }
        
        return customerUsage;
    }
    
    private Double extractDataTransferBytes(GetCostAndUsageResponse response) {
        return response.resultsByTime().stream()
            .flatMap(result -> result.groups().stream())
            .filter(group -> group.keys().contains("-DataTransfer-Out-Bytes"))
            .findFirst()
            .map(group -> group.metrics().get("UsageQuantity"))
            .map(metricValue -> Double.parseDouble(metricValue.amount()))
            .orElse(0.0);
    }
}
```

#### 2.3 UsageAggregationService

**Purpose**: Aggregate and transform usage data for API responses

```java
@Service
@Slf4j
public class UsageAggregationService {
    
    private final UsageSnapshotRepository usageSnapshotRepository;
    
    /**
     * Aggregate usage by day with 24-hour resolution
     * Selects data source based on recency
     */
    public List<DailyUsageDTO> aggregateUsageByDay(
            String customerId, Instant from, Instant to) {
        
        List<UsageSnapshot> snapshots = usageSnapshotRepository
            .findByCustomerAndTimeRange(customerId, from, to);
        
        // Group by day
        Map<LocalDate, DailyUsageDTO> dailyUsageMap = new HashMap<>();
        
        for (UsageSnapshot snapshot : snapshots) {
            LocalDate day = LocalDateTime.ofInstant(snapshot.getSnapshotTime(), 
                                                   ZoneOffset.UTC).toLocalDate();
            
            dailyUsageMap.compute(day, (k, v) -> {
                if (v == null) {
                    v = new DailyUsageDTO();
                    v.setDate(day);
                    v.setUsageGb(0.0);
                }
                v.setUsageGb(v.getUsageGb() + snapshot.getDataTransferGb());
                v.setSource(snapshot.getSource());
                return v;
            });
        }
        
        return dailyUsageMap.values().stream()
            .sorted(Comparator.comparing(DailyUsageDTO::getDate))
            .collect(Collectors.toList());
    }
    
    /**
     * Calculate total usage for a customer in a period
     */
    public Double calculateTotalUsage(String customerId, Instant from, Instant to) {
        List<UsageSnapshot> snapshots = usageSnapshotRepository
            .findByCustomerAndTimeRange(customerId, from, to);
        
        return snapshots.stream()
            .mapToDouble(UsageSnapshot::getDataTransferGb)
            .sum();
    }
    
    /**
     * Select appropriate data source based on recency
     * - Recent data (< 24h): Use CloudWatch (15min delay)
     * - Historical data (>= 24h): Use Cost Explorer (accurate)
     */
    public UsageDataSource selectDataSource(Instant dataTimestamp) {
        Instant now = Instant.now();
        Duration age = Duration.between(dataTimestamp, now);
        
        return age.toHours() < 24 
            ? UsageDataSource.CLOUDWATCH 
            : UsageDataSource.COST_EXPLORER;
    }
}
```

**Deliverables:**
- [ ] CloudWatchService with usage retrieval and threshold checking
- [ ] CostExplorerService with billing data retrieval
- [ ] UsageAggregationService with daily aggregation logic
- [ ] DTOs: DailyUsageDTO
- [ ] Unit tests for each service

---

### Phase 3: Scheduled Jobs with HA Support (60 min)

#### 3.1 ShedLock Configuration

**Add Dependencies to pom.xml:**
```xml
<dependency>
    <groupId>net.javacrumbs.shedlock</groupId>
    <artifactId>shedlock-spring</artifactId>
    <version>4.42.0</version>
</dependency>
<dependency>
    <groupId>net.javacrumbs.shedlock</groupId>
    <artifactId>shedlock-provider-jdbc-template</artifactId>
    <version>4.42.0</version>
</dependency>
```

**ShedLock Configuration Class:**
```java
@Configuration
@EnableScheduling
@EnableSchedulerLock(defaultLockAtMostFor = "PT10M")
public class SchedulingConfig {
    
    @Bean
    public LockProvider lockProvider(DataSource dataSource) {
        return new JdbcTemplateLockProvider(JdbcTemplateLockProvider.Configuration.builder()
            .withJdbcTemplate(new JdbcTemplate(dataSource))
            .usingDbTime()
            .build()
        );
    }
}
```

#### 3.2 Billing Scheduled Job

**Purpose**: Send billing events to billing system every 24 hours

```java
@Component
@Slf4j
public class BillingScheduledJob {
    
    private final CustomerRepository customerRepository;
    private final BillingEventRepository billingEventRepository;
    private final CostExplorerService costExplorerService;
    private final BillingEventPublisher billingEventPublisher;
    private final UsageMetricsService metricsService;
    
    /**
     * Runs every 24 hours at midnight UTC
     * Only one instance will execute due to ShedLock
     */
    @Scheduled(cron = "0 0 0 * * *") // Midnight UTC daily
    @SchedulerLock(
        name = "billingJob",
        lockAtMostFor = "PT30M",  // Release lock after 30 min max
        lockAtLeastFor = "PT5M"   // Hold lock for at least 5 min
    )
    public void sendBillingEvents() {
        log.info("Starting billing job execution");
        Instant jobStart = Instant.now();
        int processedCount = 0;
        int errorCount = 0;
        
        List<Customer> customers = customerRepository.findAll();
        
        for (Customer customer : customers) {
            try {
                processBillingForCustomer(customer);
                processedCount++;
            } catch (Exception e) {
                log.error("Failed to process billing for customer {}", 
                         customer.getCustomerId(), e);
                errorCount++;
            }
        }
        
        Duration elapsed = Duration.between(jobStart, Instant.now());
        log.info("Billing job completed. Processed: {}, Errors: {}, Duration: {}s",
                processedCount, errorCount, elapsed.getSeconds());
        
        metricsService.recordBillingJobExecution(processedCount, errorCount, elapsed);
    }
    
    @Transactional
    void processBillingForCustomer(Customer customer) {
        String customerId = customer.getCustomerId();
        
        // Find last billing period end, default to 24 hours ago
        Instant periodEnd = Instant.now().truncatedTo(ChronoUnit.DAYS);
        Instant periodStart = billingEventRepository
            .findLastBillingPeriodEnd(customerId)
            .orElse(periodEnd.minus(Duration.ofDays(1)));
        
        log.info("Processing billing for customer {} from {} to {}", 
                customerId, periodStart, periodEnd);
        
        // Check if billing event already exists (idempotency)
        Optional<BillingEvent> existing = billingEventRepository
            .findByCustomerIdAndPeriodStartAndPeriodEnd(customerId, periodStart, periodEnd);
        
        if (existing.isPresent()) {
            log.info("Billing event already exists for customer {} (period {} to {}), skipping",
                    customerId, periodStart, periodEnd);
            return;
        }
        
        // Calculate usage from Cost Explorer (accurate billing data)
        Map<String, Double> usage = costExplorerService
            .getCustomerUsageForBilling(periodStart, periodEnd);
        Double totalUsageGb = usage.getOrDefault(customerId, 0.0);
        
        // Create billing event record
        BillingEvent event = new BillingEvent();
        event.setCustomerId(customerId);
        event.setPeriodStart(periodStart);
        event.setPeriodEnd(periodEnd);
        event.setTrafficUsageGb(totalUsageGb);
        event.setStatus(BillingEventStatus.PENDING);
        
        billingEventRepository.save(event);
        
        // Publish to billing system
        billingEventPublisher.publishBillingEvent(event);
        
        // Mark as sent
        event.setStatus(BillingEventStatus.SENT);
        event.setSentAt(Instant.now());
        billingEventRepository.save(event);
        
        log.info("Billing event sent for customer {}: {} GB", customerId, totalUsageGb);
        metricsService.recordBillingEvent(customerId, totalUsageGb);
    }
}
```

#### 3.3 Usage Capping Scheduled Job

**Purpose**: Check usage limits and disable CDN if exceeded

```java
@Component
@Slf4j
public class UsageCappingScheduledJob {
    
    private final DistributionRepository distributionRepository;
    private final CloudWatchService cloudWatchService;
    private final CdnDisableService cdnDisableService;
    private final UsageMetricsService metricsService;
    
    // Thresholds
    private static final double LIMIT_15MIN_GB = 100.0;
    private static final double LIMIT_3HR_GB = 500.0;
    private static final Duration WINDOW_15MIN = Duration.ofMinutes(15);
    private static final Duration WINDOW_3HR = Duration.ofHours(3);
    
    /**
     * Runs every minute to check usage limits
     * Only one instance will execute due to ShedLock
     */
    @Scheduled(fixedRate = 60000) // Every 1 minute
    @SchedulerLock(
        name = "usageCappingJob",
        lockAtMostFor = "PT5M",
        lockAtLeastFor = "PT30S"
    )
    public void checkUsageLimits() {
        log.debug("Starting usage capping check");
        
        List<Distribution> activeDistributions = distributionRepository
            .findByStatus(DistributionStatus.ACTIVE);
        
        int checkedCount = 0;
        int cappedCount = 0;
        
        for (Distribution distribution : activeDistributions) {
            try {
                boolean capped = checkAndCapDistribution(distribution);
                if (capped) {
                    cappedCount++;
                }
                checkedCount++;
            } catch (Exception e) {
                log.error("Error checking distribution {}", 
                         distribution.getDistributionId(), e);
            }
        }
        
        log.debug("Usage capping check completed. Checked: {}, Capped: {}", 
                 checkedCount, cappedCount);
    }
    
    @Transactional
    boolean checkAndCapDistribution(Distribution distribution) {
        String distributionId = distribution.getDistributionId();
        
        // Check 15-minute limit
        if (cloudWatchService.exceedsThreshold(distributionId, LIMIT_15MIN_GB, WINDOW_15MIN)) {
            double usage = cloudWatchService.getUsageForWindow(distributionId, WINDOW_15MIN);
            String reason = String.format("Exceeded 100 GB limit in 15 minutes (%.2f GB)", usage);
            disableDistribution(distribution, reason, usage);
            metricsService.recordCappingEvent("15min", distributionId, usage);
            return true;
        }
        
        // Check 3-hour limit
        if (cloudWatchService.exceedsThreshold(distributionId, LIMIT_3HR_GB, WINDOW_3HR)) {
            double usage = cloudWatchService.getUsageForWindow(distributionId, WINDOW_3HR);
            String reason = String.format("Exceeded 500 GB limit in 3 hours (%.2f GB)", usage);
            disableDistribution(distribution, reason, usage);
            metricsService.recordCappingEvent("3hr", distributionId, usage);
            return true;
        }
        
        return false;
    }
    
    private void disableDistribution(Distribution distribution, String reason, double usageGb) {
        log.warn("Disabling distribution {}: {}", distribution.getDistributionId(), reason);
        
        distribution.setStatus(DistributionStatus.DISABLED);
        distribution.setDisabledAt(Instant.now());
        distribution.setDisableReason(reason);
        distributionRepository.save(distribution);
        
        cdnDisableService.disableDistribution(
            distribution.getDistributionId(), reason, usageGb
        );
    }
}
```

#### 3.4 Data Collection Scheduled Job

**Purpose**: Periodically collect usage data from AWS sources

```java
@Component
@Slf4j
public class DataCollectionScheduledJob {
    
    private final CloudWatchService cloudWatchService;
    private final CostExplorerService costExplorerService;
    private final UsageMetricsService metricsService;
    
    /**
     * Runs every 5 minutes to collect CloudWatch data
     */
    @Scheduled(fixedRate = 300000) // Every 5 minutes
    @SchedulerLock(
        name = "dataCollectionJob",
        lockAtMostFor = "PT10M",
        lockAtLeastFor = "PT2M"
    )
    public void collectUsageData() {
        log.debug("Starting data collection job");
        
        try {
            // Collect CloudWatch data (last 3 hours)
            Instant to = Instant.now();
            Instant from = to.minus(Duration.ofHours(3));
            
            List<UsageSnapshot> cloudWatchSnapshots = 
                cloudWatchService.retrieveAndStoreRecentUsage(from, to);
            
            log.info("Collected {} CloudWatch snapshots", cloudWatchSnapshots.size());
            metricsService.recordDataCollection("cloudwatch", cloudWatchSnapshots.size());
            
        } catch (Exception e) {
            log.error("Failed to collect CloudWatch data", e);
        }
    }
    
    /**
     * Runs daily to collect Cost Explorer data for yesterday
     * (Cost Explorer data is delayed by ~24 hours)
     */
    @Scheduled(cron = "0 30 1 * * *") // 1:30 AM UTC daily
    @SchedulerLock(
        name = "costExplorerCollectionJob",
        lockAtMostFor = "PT30M",
        lockAtLeastFor = "PT5M"
    )
    public void collectBillingData() {
        log.info("Starting Cost Explorer data collection");
        
        try {
            // Collect yesterday's data (24h delay)
            Instant to = Instant.now().minus(Duration.ofDays(1))
                .truncatedTo(ChronoUnit.DAYS);
            Instant from = to.minus(Duration.ofDays(1));
            
            List<UsageSnapshot> costExplorerSnapshots = 
                costExplorerService.retrieveAndStoreBillingUsage(from, to);
            
            log.info("Collected {} Cost Explorer snapshots", costExplorerSnapshots.size());
            metricsService.recordDataCollection("costexplorer", costExplorerSnapshots.size());
            
        } catch (Exception e) {
            log.error("Failed to collect Cost Explorer data", e);
        }
    }
}
```

**Deliverables:**
- [ ] ShedLock configuration for HA
- [ ] BillingScheduledJob (24h cadence)
- [ ] UsageCappingScheduledJob (1min cadence)
- [ ] DataCollectionScheduledJob (5min cadence)
- [ ] Proper transaction management
- [ ] Comprehensive error handling

---

### Phase 4: REST API Implementation (30 min)

#### 4.1 Usage Controller

```java
@RestController
@RequestMapping("/api/usage")
@Slf4j
public class UsageController {
    
    private final UsageAggregationService aggregationService;
    private final CustomerRepository customerRepository;
    
    /**
     * Get usage data for a customer
     * Header: X-Customer-Id (required)
     * Params: from (optional), to (optional)
     * Returns: Daily usage with 24-hour resolution
     */
    @GetMapping
    public ResponseEntity<UsageResponse> getUsage(
            @RequestHeader("X-Customer-Id") String customerId,
            @RequestParam(required = false) 
            @DateTimeFormat(iso = DateTimeFormat.ISO.DATE_TIME) Instant from,
            @RequestParam(required = false) 
            @DateTimeFormat(iso = DateTimeFormat.ISO.DATE_TIME) Instant to) {
        
        log.info("Usage request for customer {}, from: {}, to: {}", customerId, from, to);
        
        // Validate customer exists
        Customer customer = customerRepository.findById(customerId)
            .orElseThrow(() -> new CustomerNotFoundException(customerId));
        
        // Default to last 30 days if not specified
        if (to == null) {
            to = Instant.now();
        }
        if (from == null) {
            from = to.minus(Duration.ofDays(30));
        }
        
        // Validate date range
        if (from.isAfter(to)) {
            throw new InvalidDateRangeException("'from' must be before 'to'");
        }
        
        // Aggregate usage by day
        List<DailyUsageDTO> dailyUsage = aggregationService
            .aggregateUsageByDay(customerId, from, to);
        
        Double totalUsage = aggregationService
            .calculateTotalUsage(customerId, from, to);
        
        UsageResponse response = UsageResponse.builder()
            .customerId(customerId)
            .periodStart(from)
            .periodEnd(to)
            .totalUsageGb(totalUsage)
            .dailyUsage(dailyUsage)
            .build();
        
        log.info("Returning usage for customer {}: {} GB", customerId, totalUsage);
        return ResponseEntity.ok(response);
    }
}
```

#### 4.2 DTOs

```java
@Data
@Builder
public class UsageResponse {
    private String customerId;
    private Instant periodStart;
    private Instant periodEnd;
    private Double totalUsageGb;
    private List<DailyUsageDTO> dailyUsage;
}

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DailyUsageDTO {
    private LocalDate date;
    private Double usageGb;
    private UsageDataSource source; // CLOUDWATCH or COST_EXPLORER
}

@Data
@Builder
public class ErrorResponse {
    private int status;
    private String error;
    private String message;
    private Instant timestamp;
}
```

#### 4.3 Exception Handling

```java
@ControllerAdvice
@Slf4j
public class GlobalExceptionHandler {
    
    @ExceptionHandler(CustomerNotFoundException.class)
    public ResponseEntity<ErrorResponse> handleCustomerNotFound(CustomerNotFoundException ex) {
        log.warn("Customer not found: {}", ex.getMessage());
        
        ErrorResponse error = ErrorResponse.builder()
            .status(HttpStatus.NOT_FOUND.value())
            .error("Customer Not Found")
            .message(ex.getMessage())
            .timestamp(Instant.now())
            .build();
        
        return ResponseEntity.status(HttpStatus.NOT_FOUND).body(error);
    }
    
    @ExceptionHandler(InvalidDateRangeException.class)
    public ResponseEntity<ErrorResponse> handleInvalidDateRange(InvalidDateRangeException ex) {
        log.warn("Invalid date range: {}", ex.getMessage());
        
        ErrorResponse error = ErrorResponse.builder()
            .status(HttpStatus.BAD_REQUEST.value())
            .error("Invalid Date Range")
            .message(ex.getMessage())
            .timestamp(Instant.now())
            .build();
        
        return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(error);
    }
    
    @ExceptionHandler(MissingRequestHeaderException.class)
    public ResponseEntity<ErrorResponse> handleMissingHeader(MissingRequestHeaderException ex) {
        log.warn("Missing required header: {}", ex.getMessage());
        
        ErrorResponse error = ErrorResponse.builder()
            .status(HttpStatus.BAD_REQUEST.value())
            .error("Missing Header")
            .message("X-Customer-Id header is required")
            .timestamp(Instant.now())
            .build();
        
        return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(error);
    }
    
    @ExceptionHandler(Exception.class)
    public ResponseEntity<ErrorResponse> handleGenericException(Exception ex) {
        log.error("Unexpected error", ex);
        
        ErrorResponse error = ErrorResponse.builder()
            .status(HttpStatus.INTERNAL_SERVER_ERROR.value())
            .error("Internal Server Error")
            .message("An unexpected error occurred")
            .timestamp(Instant.now())
            .build();
        
        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(error);
    }
}
```

#### 4.4 Custom Exceptions

```java
public class CustomerNotFoundException extends RuntimeException {
    public CustomerNotFoundException(String customerId) {
        super("Customer not found: " + customerId);
    }
}

public class InvalidDateRangeException extends RuntimeException {
    public InvalidDateRangeException(String message) {
        super(message);
    }
}
```

**Deliverables:**
- [ ] UsageController with GET /api/usage endpoint
- [ ] Request/Response DTOs
- [ ] Global exception handler
- [ ] Custom exceptions
- [ ] Request validation
- [ ] Integration tests

---

### Phase 5: Integration Layer (20 min)

#### 5.1 Billing Event Publisher

```java
@Service
@Slf4j
public class BillingEventPublisher {
    
    /**
     * Publish billing event to billing system
     * In production: Would use Kafka/RabbitMQ
     * For challenge: Print to console
     */
    public void publishBillingEvent(BillingEvent event) {
        String json = formatBillingEvent(event);
        
        // Mock: Print to console
        System.out.println("=====================================================");
        System.out.println("BILLING EVENT PUBLISHED");
        System.out.println("=====================================================");
        System.out.println(json);
        System.out.println("=====================================================");
        
        log.info("Published billing event for customer {} (period {} to {})", 
                event.getCustomerId(), event.getPeriodStart(), event.getPeriodEnd());
        
        // In production:
        // kafkaTemplate.send("billing-events", event.getCustomerId(), json);
    }
    
    private String formatBillingEvent(BillingEvent event) {
        return String.format("""
            {
              "type": "EVENT_CDN_USAGE",
              "payload": {
                "id": "%s",
                "customerId": "%s",
                "startPeriod": "%s",
                "endPeriod": "%s",
                "trafficUsageGb": "%.2f"
              }
            }
            """,
            event.getId(),
            event.getCustomerId(),
            event.getPeriodStart().toString(),
            event.getPeriodEnd().toString(),
            event.getTrafficUsageGb()
        );
    }
}
```

#### 5.2 CDN Disable Service

```java
@Service
@Slf4j
public class CdnDisableService {
    
    /**
     * Disable CDN distribution due to usage limit exceeded
     * In production: Would call AWS CloudFront API to disable distribution
     * For challenge: Mock with status update and event publication
     */
    public void disableDistribution(String distributionId, String reason, Double usageGb) {
        String event = formatDisableEvent(distributionId, reason, usageGb);
        
        // Mock: Print to console
        System.out.println("=====================================================");
        System.out.println("CDN DISABLE EVENT");
        System.out.println("=====================================================");
        System.out.println(event);
        System.out.println("=====================================================");
        
        log.warn("CDN distribution {} disabled: {}", distributionId, reason);
        
        // In production:
        // 1. Call AWS CloudFront to disable distribution
        // cloudFrontClient.updateDistribution(distributionId, enabled=false);
        // 2. Publish event to message broker
        // kafkaTemplate.send("cdn-events", distributionId, event);
    }
    
    private String formatDisableEvent(String distributionId, String reason, Double usageGb) {
        return String.format("""
            {
              "type": "EVENT_CDN_DISABLED",
              "payload": {
                "distributionId": "%s",
                "reason": "%s",
                "usageGb": "%.2f",
                "timestamp": "%s"
              }
            }
            """,
            distributionId,
            reason,
            usageGb,
            Instant.now().toString()
        );
    }
}
```

**Deliverables:**
- [ ] BillingEventPublisher with console output
- [ ] CdnDisableService with console output
- [ ] Proper event formatting
- [ ] Production-ready comments for Kafka integration

---

### Phase 6: Observability & Monitoring (20 min)

#### 6.1 Metrics Service

```java
@Service
public class UsageMetricsService {
    
    private final MeterRegistry meterRegistry;
    
    public UsageMetricsService(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
    }
    
    public void recordBillingEvent(String customerId, Double usageGb) {
        meterRegistry.counter("cdn.billing.events.sent", 
            "customer", customerId).increment();
        meterRegistry.gauge("cdn.billing.latest.usage.gb", 
            Tags.of("customer", customerId), usageGb);
    }
    
    public void recordBillingJobExecution(int processed, int errors, Duration duration) {
        meterRegistry.counter("cdn.billing.job.executions").increment();
        meterRegistry.counter("cdn.billing.job.customers.processed").increment(processed);
        meterRegistry.counter("cdn.billing.job.errors").increment(errors);
        meterRegistry.timer("cdn.billing.job.duration").record(duration);
    }
    
    public void recordCappingEvent(String reason, String distributionId, Double usageGb) {
        meterRegistry.counter("cdn.capping.events", 
            "reason", reason,
            "distribution", distributionId).increment();
        meterRegistry.gauge("cdn.capping.usage.gb",
            Tags.of("distribution", distributionId), usageGb);
    }
    
    public void recordDataCollection(String source, int snapshotCount) {
        meterRegistry.counter("cdn.data.collection.snapshots",
            "source", source).increment(snapshotCount);
    }
    
    public void recordApiRequest(String customerId, Duration responseTime) {
        meterRegistry.counter("cdn.api.requests",
            "customer", customerId).increment();
        meterRegistry.timer("cdn.api.response.time").record(responseTime);
    }
}
```

#### 6.2 Health Indicators

```java
@Component
public class AwsDataSourceHealthIndicator implements HealthIndicator {
    
    private final CloudWatchClient cloudWatchClient;
    private final CostExplorerClient costExplorerClient;
    
    @Override
    public Health health() {
        try {
            // Test CloudWatch connectivity
            Instant now = Instant.now();
            cloudWatchClient.getMetrics(now.minus(Duration.ofMinutes(5)), now);
            
            // Test Cost Explorer connectivity
            costExplorerClient.retrieveUsage(
                now.minus(Duration.ofDays(2)), 
                now.minus(Duration.ofDays(1)), 
                "test-dist"
            );
            
            return Health.up()
                .withDetail("cloudwatch", "available")
                .withDetail("costExplorer", "available")
                .withDetail("timestamp", Instant.now())
                .build();
                
        } catch (Exception e) {
            return Health.down()
                .withDetail("error", e.getMessage())
                .withDetail("timestamp", Instant.now())
                .withException(e)
                .build();
        }
    }
}
```

#### 6.3 Request Logging Interceptor

```java
@Component
public class RequestLoggingInterceptor implements HandlerInterceptor {
    
    private static final Logger log = LoggerFactory.getLogger(RequestLoggingInterceptor.class);
    private static final String START_TIME_ATTR = "startTime";
    
    @Override
    public boolean preHandle(HttpServletRequest request, 
                            HttpServletResponse response, 
                            Object handler) {
        request.setAttribute(START_TIME_ATTR, Instant.now());
        
        log.info("API Request: {} {} - Customer: {}", 
                request.getMethod(),
                request.getRequestURI(),
                request.getHeader("X-Customer-Id"));
        
        return true;
    }
    
    @Override
    public void afterCompletion(HttpServletRequest request, 
                               HttpServletResponse response, 
                               Object handler, 
                               Exception ex) {
        Instant startTime = (Instant) request.getAttribute(START_TIME_ATTR);
        Duration duration = Duration.between(startTime, Instant.now());
        
        log.info("API Response: {} {} - Status: {} - Duration: {}ms",
                request.getMethod(),
                request.getRequestURI(),
                response.getStatus(),
                duration.toMillis());
    }
}

@Configuration
public class WebConfig implements WebMvcConfigurer {
    
    @Autowired
    private RequestLoggingInterceptor requestLoggingInterceptor;
    
    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(requestLoggingInterceptor);
    }
}
```

#### 6.4 Application Configuration

**application.properties:**
```properties
# Application
spring.application.name=cdn-challenge

# Database (H2)
spring.datasource.url=jdbc:h2:mem:cdndb
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.jpa.hibernate.ddl-auto=none
spring.sql.init.mode=always
spring.sql.init.schema-locations=classpath:schema.sql
spring.sql.init.data-locations=classpath:data.sql

# H2 Console (for debugging)
spring.h2.console.enabled=true
spring.h2.console.path=/h2-console

# Logging
logging.level.root=INFO
logging.level.com.bitmovin.platform.challenge=DEBUG
logging.level.org.springframework.scheduling=DEBUG
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n

# Actuator
management.endpoints.web.exposure.include=health,metrics,info,scheduledtasks
management.endpoint.health.show-details=always
management.metrics.export.prometheus.enabled=true

# Scheduling
spring.task.scheduling.pool.size=5
```

**Deliverables:**
- [ ] UsageMetricsService with custom metrics
- [ ] AwsDataSourceHealthIndicator
- [ ] Request logging interceptor
- [ ] application.properties configuration
- [ ] Actuator endpoints exposed

---

## ğŸ”„ HIGH AVAILABILITY STRATEGY

### Challenge: Multiple Instances Running Concurrently

**Problems to Solve:**
1. **Duplicate Billing Events**: Two instances might send the same billing event
2. **Duplicate Capping Actions**: Two instances might disable the same distribution
3. **Data Consistency**: Race conditions when updating distribution status

### Solutions Implemented:

#### 1. ShedLock (Distributed Locking)
- **Mechanism**: Database-based pessimistic locking
- **Table**: `shedlock` with columns: `name`, `lock_until`, `locked_at`, `locked_by`
- **Behavior**: Only one instance can acquire lock and execute scheduled job
- **Configuration**:
  - `lockAtMostFor`: Maximum lock duration (fail-safe)
  - `lockAtLeastFor`: Minimum lock duration (prevent rapid re-execution)

**Lock Strategy by Job:**
| Job | Lock Name | At Most For | At Least For | Rationale |
|-----|-----------|-------------|--------------|-----------|
| BillingJob | billingJob | 30 min | 5 min | Long-running, process all customers |
| CappingJob | usageCappingJob | 5 min | 30 sec | Frequent checks, quick execution |
| DataCollectionJob | dataCollectionJob | 10 min | 2 min | API calls to AWS, moderate duration |

#### 2. Database Constraints (Idempotency)
- **Unique Constraint**: `billing_event(customer_id, period_start, period_end)`
- **Effect**: Database prevents duplicate billing events at DB level
- **Behavior**: If two instances try to insert same billing event, one will fail with constraint violation

#### 3. Optimistic Locking (Distribution Status)
- **@Version**: JPA version field on `Distribution` entity
- **Behavior**: If two instances try to update same distribution, one will fail with `OptimisticLockException`
- **Handling**: Retry logic or accept failure (distribution already disabled)

### HA Verification Checklist:
- [ ] ShedLock table created in schema.sql
- [ ] @EnableSchedulerLock configured
- [ ] All @Scheduled methods have @SchedulerLock annotation
- [ ] Unique constraints on BillingEvent
- [ ] @Version field on Distribution entity
- [ ] Transaction boundaries properly defined

### Discussion Points for Interview:
1. **ShedLock failure**: What happens if DB goes down during lock acquisition?
   - Answer: Scheduled jobs won't run until DB recovers (fail-safe)
2. **Lock timeout**: What if job takes longer than `lockAtMostFor`?
   - Answer: Lock is released, another instance might start (use conservative timeout)
3. **Alternative approaches**:
   - Quartz Scheduler with JDBC JobStore (heavier, more features)
   - Redis-based locking (requires Redis infrastructure)
   - Leader election (Kubernetes, ZooKeeper)

---

## âš ï¸ CHALLENGES & MITIGATIONS

### Challenge 1: Sliding Window Calculations

**Problem**: Efficiently calculate usage over sliding 15-minute and 3-hour windows.

**Approach**:
1. Store usage snapshots every 5 minutes from CloudWatch
2. Query snapshots within time window using indexed queries
3. Sum data transfer GB from matching snapshots

**Optimization Considerations**:
- Index on `(distribution_id, snapshot_time, source)` for fast lookups
- Consider caching recent snapshots in memory (future enhancement)
- Pre-aggregate at different granularities (1min, 5min, 15min) for faster queries

**Query Example**:
```sql
SELECT SUM(data_transfer_gb) 
FROM usage_snapshot 
WHERE distribution_id = 'dist-1' 
  AND snapshot_time >= NOW() - INTERVAL 15 MINUTE
  AND source = 'CLOUDWATCH'
```

---

### Challenge 2: Data Source Selection

**Problem**: Choose between CloudWatch (recent, approximate) and Cost Explorer (delayed, accurate).

**Strategy**:
```
IF data_age < 24 hours THEN
    Use CloudWatch (15-minute delay acceptable)
ELSE
    Use Cost Explorer (billing-accurate)
END IF
```

**Implementation**: `UsageAggregationService.selectDataSource()`

**Edge Cases**:
- What if CloudWatch data is missing? â†’ Return empty, log warning
- What if Cost Explorer data is not yet available? â†’ Use CloudWatch with warning
- Hybrid approach: Merge both sources with priority to Cost Explorer

---

### Challenge 3: Time Zone Handling

**Problem**: Billing periods, customer time zones, UTC vs local time.

**Solution**:
- Store all timestamps as `Instant` (UTC)
- Billing periods aligned to midnight UTC
- API responses include ISO 8601 timestamps with Z suffix
- Convert to customer timezone only for display (future enhancement)

---

### Challenge 4: Mock Data Limitations

**Problem**: Provided mocks have hardcoded UUIDs that don't match database seeds.

**Mitigation**:
1. **Option A**: Update seed data to use UUIDs from mocks
2. **Option B**: Modify CloudWatchClient/CostExplorerClient constructor to accept custom mock data
3. **Option C**: Create wrapper services that map between mock IDs and DB IDs

**Recommended**: Option B - Allows flexible testing with custom data

---

### Challenge 5: Testing Scheduled Jobs

**Problem**: Hard to test time-based logic without waiting for scheduled execution.

**Solutions**:
1. **Extract Business Logic**: Move logic out of @Scheduled methods into separate service methods
2. **Disable Scheduling in Tests**: Use `@ActiveProfiles("test")` with conditional scheduling
3. **Manual Trigger**: Create test endpoints that trigger jobs manually (disabled in production)
4. **Mock Clock**: Inject `Clock` bean and mock it in tests

**Test Configuration**:
```java
@TestConfiguration
public class TestSchedulingConfig {
    @Bean
    @Primary
    public Clock fixedClock() {
        return Clock.fixed(Instant.parse("2025-10-23T12:00:00Z"), ZoneId.of("UTC"));
    }
}
```

---

## ğŸ§ª TESTING STRATEGY

### Unit Tests

**CloudWatchService:**
- [ ] Test usage retrieval and storage
- [ ] Test threshold checking (below, at, above limit)
- [ ] Test window calculations (15min, 3hr)
- [ ] Mock CloudWatchClient responses

**CostExplorerService:**
- [ ] Test billing data retrieval
- [ ] Test customer usage aggregation
- [ ] Mock CostExplorerClient responses

**UsageAggregationService:**
- [ ] Test daily aggregation logic
- [ ] Test data source selection
- [ ] Test total usage calculation

**Scheduled Jobs:**
- [ ] Test business logic separately from @Scheduled
- [ ] Test billing event creation and idempotency
- [ ] Test capping logic and threshold detection
- [ ] Mock all external dependencies

### Integration Tests

**UsageController:**
- [ ] Test GET /api/usage with valid customer
- [ ] Test missing X-Customer-Id header â†’ 400
- [ ] Test invalid customer ID â†’ 404
- [ ] Test date range validation
- [ ] Test response format

**Database:**
- [ ] Test custom queries (findByCustomerAndTimeRange, etc.)
- [ ] Test unique constraints (duplicate billing events)
- [ ] Test optimistic locking on Distribution

**End-to-End:**
- [ ] Full flow: Data collection â†’ Aggregation â†’ API response
- [ ] Full flow: Usage exceeds limit â†’ Distribution disabled â†’ Event published
- [ ] Full flow: Billing job execution â†’ Event sent â†’ DB updated

### Test Data Setup

```java
@TestConfiguration
public class TestDataConfig {
    
    @Bean
    public CloudWatchClient testCloudWatchClient() {
        return new CloudWatchClient(List.of(
            new RecordedMetricUsage("dist-1", 60.0),
            new RecordedMetricUsage("dist-2", 120.0),
            new RecordedMetricUsage("dist-3", 30.0)
        ));
    }
    
    @Bean
    public CostExplorerClient testCostExplorerClient() {
        return new CostExplorerClient(List.of(
            new RecordedBillingUsage("dist-1", 100.0, 500.5),
            new RecordedBillingUsage("dist-2", 50.0, 250.3),
            new RecordedBillingUsage("dist-3", 75.0, 800.7)
        ));
    }
}
```

---

## ğŸ“Š OBSERVABILITY PLAN

### Logging Strategy

**Log Levels:**

**INFO:**
- Scheduled job start/completion with summary
- Billing events sent (customer, amount)
- CDN disablement (distribution, reason)
- API requests (method, endpoint, customer)

**DEBUG:**
- Detailed usage calculations
- Data source selection decisions
- Individual snapshot storage
- Query execution details

**WARN:**
- Usage approaching limits (80%, 90%)
- Missing data from AWS sources
- Stale data detection
- Failed idempotency checks (already processed)

**ERROR:**
- AWS client failures with stack traces
- Database errors
- Unexpected exceptions in jobs
- Data inconsistencies

### Metrics to Expose

**Business Metrics:**
```
cdn.billing.events.sent{customer}          - Counter
cdn.billing.latest.usage.gb{customer}      - Gauge
cdn.capping.events{reason,distribution}    - Counter
cdn.distributions.active                    - Gauge
cdn.distributions.disabled                  - Gauge
cdn.api.requests{customer,endpoint}        - Counter
```

**Technical Metrics:**
```
cdn.billing.job.executions                 - Counter
cdn.billing.job.duration                   - Timer
cdn.billing.job.errors                     - Counter
cdn.data.collection.snapshots{source}      - Counter
cdn.api.response.time                      - Timer
```

**JVM Metrics (via Actuator):**
- Memory usage
- Thread count
- GC statistics

### Health Checks

**Endpoints:**
- `/actuator/health` - Overall health
- `/actuator/health/db` - Database connectivity
- `/actuator/health/awsDataSource` - Custom AWS health check
- `/actuator/metrics` - All metrics
- `/actuator/scheduledtasks` - View scheduled jobs

**Custom Health Check:**
```java
@Component
public class AwsDataSourceHealthIndicator implements HealthIndicator {
    @Override
    public Health health() {
        // Test CloudWatch and Cost Explorer connectivity
    }
}
```

---

## âœ… IMPLEMENTATION CHECKLIST

### Phase 1: Domain Model & Database âœ“
- [ ] Create Customer entity
- [ ] Create Distribution entity with @Version
- [ ] Create UsageSnapshot entity with indexes
- [ ] Create BillingEvent entity with unique constraint
- [ ] Create all repositories with custom queries
- [ ] Create schema.sql with all tables and indexes
- [ ] Create data.sql with seed data
- [ ] Create enums: DistributionStatus, UsageDataSource, BillingEventStatus

### Phase 2: AWS Data Retrieval âœ“
- [ ] Implement CloudWatchService
- [ ] Implement CostExplorerService
- [ ] Implement UsageAggregationService
- [ ] Create DTOs: DailyUsageDTO
- [ ] Add error handling for AWS failures
- [ ] Write unit tests for all services

### Phase 3: Scheduled Jobs âœ“
- [ ] Add ShedLock dependencies to pom.xml
- [ ] Create SchedulingConfig with @EnableSchedulerLock
- [ ] Implement BillingScheduledJob with @SchedulerLock
- [ ] Implement UsageCappingScheduledJob with @SchedulerLock
- [ ] Implement DataCollectionScheduledJob with @SchedulerLock
- [ ] Add proper transaction boundaries
- [ ] Add comprehensive logging
- [ ] Test idempotency scenarios

### Phase 4: REST API âœ“
- [ ] Implement UsageController
- [ ] Create UsageResponse DTO
- [ ] Implement GlobalExceptionHandler
- [ ] Create custom exceptions (CustomerNotFoundException, etc.)
- [ ] Add request validation
- [ ] Add RequestLoggingInterceptor
- [ ] Write integration tests

### Phase 5: Integration Layer âœ“
- [ ] Implement BillingEventPublisher
- [ ] Implement CdnDisableService
- [ ] Format events according to specification
- [ ] Add production-ready comments for Kafka

### Phase 6: Observability âœ“
- [ ] Implement UsageMetricsService
- [ ] Implement AwsDataSourceHealthIndicator
- [ ] Configure application.properties
- [ ] Configure actuator endpoints
- [ ] Add structured logging

### Final Verification âœ“
- [ ] Run application and verify startup
- [ ] Test H2 console connectivity
- [ ] Test REST API with curl/Postman
- [ ] Verify scheduled jobs execute
- [ ] Check actuator endpoints
- [ ] Review logs for errors
- [ ] Verify metrics are exposed

---

## ğŸš€ IMPLEMENTATION ORDER

**Recommended Sequence:**

1. **Database Foundation** (30 min)
   - Entities â†’ Repositories â†’ Schema â†’ Seeds
   
2. **AWS Integration** (45 min)
   - CloudWatchService â†’ CostExplorerService â†’ Aggregation
   
3. **Scheduled Jobs** (60 min)
   - ShedLock setup â†’ DataCollectionJob â†’ CappingJob â†’ BillingJob
   
4. **REST API** (30 min)
   - Controller â†’ DTOs â†’ Exception Handling
   
5. **Integration** (20 min)
   - Publishers â†’ Event Formatting
   
6. **Observability** (20 min)
   - Metrics â†’ Health â†’ Logging â†’ Configuration
   
7. **Testing & Polish** (15 min)
   - Manual testing â†’ Fix bugs â†’ Documentation

**Total: 3h 40min** (20min buffer)

---

## ğŸ“š KEY DISCUSSION POINTS FOR INTERVIEW

### Architecture Decisions:
1. **Why ShedLock over Quartz?**
   - Simpler setup, sufficient for use case
   - No additional infrastructure required
   - Database-based locking fits existing stack

2. **Why H2 over PostgreSQL?**
   - Challenge requirement for quick setup
   - Production would use PostgreSQL/MySQL

3. **Data Source Selection Strategy**
   - CloudWatch for recent data (< 24h, 15min delay)
   - Cost Explorer for billing (> 24h, accurate)
   - Hybrid approach possible for better accuracy

### Scalability Considerations:
1. **Database becomes bottleneck?**
   - Add read replicas for API queries
   - Separate billing DB from operational DB
   - Cache recent usage data in Redis

2. **High cardinality metrics?**
   - Limit customer tag values
   - Aggregate metrics at different levels
   - Use sampling for high-volume metrics

3. **Scheduled job optimization?**
   - Parallel processing per customer
   - Batch operations in groups
   - Partition work across multiple jobs

### Production Enhancements:
1. **Message Broker Integration**
   - Replace System.out with Kafka producer
   - Add retry logic and dead letter queues
   - Implement event schemas with Avro

2. **Monitoring & Alerting**
   - Grafana dashboards for metrics
   - PagerDuty alerts for capping events
   - CloudWatch alarms for job failures

3. **Security**
   - OAuth2 authentication for API
   - Rate limiting per customer
   - Audit logging for all actions

---

## ğŸ¯ SUCCESS CRITERIA

**Functional Requirements Met:**
- âœ… Billing events sent every 24 hours
- âœ… REST API exposes usage with 15-minute delay
- âœ… CDN disabled when limits exceeded
- âœ… Dual data sources properly utilized

**Non-Functional Requirements Met:**
- âœ… HA mode supported with ShedLock
- âœ… Idempotency guaranteed
- âœ… Comprehensive logging implemented
- âœ… Metrics exposed via Actuator

**Code Quality:**
- âœ… Clean architecture (layers, separation of concerns)
- âœ… Proper error handling
- âœ… Transaction management
- âœ… Unit and integration tests

**Interview Performance:**
- âœ… Clear explanation of design decisions
- âœ… Discussion of trade-offs
- âœ… Awareness of production considerations
- âœ… Problem-solving approach demonstrated

---

**STATUS**: âœ… PLAN COMPLETE - READY FOR IMPLEMENTATION

**NEXT MODE**: IMPLEMENT MODE

**ESTIMATED COMPLETION**: 4 hours
